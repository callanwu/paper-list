- Hallucination
  - [Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models](https://arxiv.org/pdf/2309.01219.pdf)
  - [LLM Lies: Hallucinations are not Bugs, but Features as Adversarial Examples](https://arxiv.org/pdf/2310.01469.pdf)

- Retrieval-Augmented Generation
  - [Benchmarking Large Language Models in Retrieval-Augmented Generation](https://arxiv.org/pdf/2309.01431.pdf)
  - [Query Rewriting for Retrieval-Augmented Large Language Models](https://arxiv.org/pdf/2305.14283.pdf)
  - [LLM Self-Reflective Retrieval-Augmented Generation](https://arxiv.org/pdf/2310.11511.pdf)
- Parameter-Efficient Fine-Tuning
  - [LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention](https://arxiv.org/pdf/2303.16199.pdf)
  - [LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models](https://browse.arxiv.org/pdf/2309.12307.pdf)
- Attack
  - [Universal and Transferable Adversarial Attacks on Aligned Language Models](https://arxiv.org/pdf/2307.15043.pdf)
  - [AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models](https://arxiv.org/pdf/2310.04451.pdf)
  - [Catastrophic (ðŸ˜”) Jailbreak of Open-source LLMs via Exploiting Generation](https://arxiv.org/pdf/2310.06987.pdf)
  - [LLM Lies: Hallucinations are not Bugs, but Features as Adversarial Examples](https://arxiv.org/pdf/2310.01469.pdf)
  
**Jailbreak in LLM is really intersting, there isn't a repo to collect related these papers yet, I'll start a separate repo to collect them afterward.**
- Model Editing
  - [Editing Large Language Models: Problems, Methods, and Opportunities](https://arxiv.org/pdf/2305.13172.pdf)
  - [Can We Edit Multimodal Large Language Models?](https://arxiv.org/pdf/2310.08475.pdf)
  - [Locating and Editing Factual Associations in GPT](https://arxiv.org/pdf/2202.05262.pdf)
  - [Emptying the Ocean with a Spoon: Should We Edit Models?](https://arxiv.org/pdf/2310.11958.pdf)
- Causal Inference
  - [Psychologically-Inspired Causal Prompts](https://arxiv.org/pdf/2305.01764.pdf)
- Fun
  - [Pretraining on the Test Set Is All You Need](https://arxiv.org/pdf/2309.08632.pdf)