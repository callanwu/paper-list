- Hallucination
  - [Siren's Song in the AI Ocean: A Survey on Hallucination in Large Language Models](https://arxiv.org/pdf/2309.01219.pdf)
  - [LLM Lies: Hallucinations are not Bugs, but Features as Adversarial Examples](https://arxiv.org/pdf/2310.01469.pdf)

- Retrieval-Augmented Generation
  - [Benchmarking Large Language Models in Retrieval-Augmented Generation](https://arxiv.org/pdf/2309.01431.pdf)
  - [Query Rewriting for Retrieval-Augmented Large Language Models](https://arxiv.org/pdf/2305.14283.pdf)
  - [LLM Self-Reflective Retrieval-Augmented Generation](https://arxiv.org/pdf/2310.11511.pdf)
  - [Learning to Filter Context for Retrieval-Augmented Generation](https://arxiv.org/pdf/2311.08377.pdf)

- Parameter-Efficient Fine-Tuning
  - [LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention](https://arxiv.org/pdf/2303.16199.pdf)
  - [LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models](https://browse.arxiv.org/pdf/2309.12307.pdf)
  - [LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models](https://arxiv.org/pdf/2304.01933.pdf)
  - [Towards a Unified View of Parameter-Efficient Transfer Learning](https://arxiv.org/pdf/2110.04366.pdf)
  - [LoftQ: LoRA-Fine-Tuning-Aware Quantization for Large Language Models](https://arxiv.org/pdf/2310.08659.pdf)

- Attack
  - [Universal and Transferable Adversarial Attacks on Aligned Language Models](https://arxiv.org/pdf/2307.15043.pdf)
  - [AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models](https://arxiv.org/pdf/2310.04451.pdf)
  - [Catastrophic (ðŸ˜”) Jailbreak of Open-source LLMs via Exploiting Generation](https://arxiv.org/pdf/2310.06987.pdf)
  - [LLM Lies: Hallucinations are not Bugs, but Features as Adversarial Examples](https://arxiv.org/pdf/2310.01469.pdf)
  - [Fine-tuning Aligned Language Models Compromises Safety,Even When Users Do Not Intend To!](https://arxiv.org/pdf/2310.03693.pdf)
  - [Language Model Inversion](https://openreview.net/pdf?id=t9dWHpGkPj)
**Jailbreak in LLM is really intersting, there isn't a repo to collect related these papers yet, I'll start a separate repo to collect them afterward.**
- Model Editing
  - [Editing Large Language Models: Problems, Methods, and Opportunities](https://arxiv.org/pdf/2305.13172.pdf)
  - [Can We Edit Multimodal Large Language Models?](https://arxiv.org/pdf/2310.08475.pdf)
  - [Locating and Editing Factual Associations in GPT](https://arxiv.org/pdf/2202.05262.pdf)
  - [Emptying the Ocean with a Spoon: Should We Edit Models?](https://arxiv.org/pdf/2310.11958.pdf)
- Causal Inference
  - [Psychologically-Inspired Causal Prompts](https://arxiv.org/pdf/2305.01764.pdf)
  - [Causal Intervention and Counterfactual Reasoning for Multi-modal Fake News Detection](https://aclanthology.org/2023.acl-long.37.pdf)
  - [Contextual Debiasing for Visual Recognition with Causal Mechanisms](https://openaccess.thecvf.com/content/CVPR2022/papers/Liu_Contextual_Debiasing_for_Visual_Recognition_With_Causal_Mechanisms_CVPR_2022_paper.pdf)
- Fun
  - [Pretraining on the Test Set Is All You Need](https://arxiv.org/pdf/2309.08632.pdf)
